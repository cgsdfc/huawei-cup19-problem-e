{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74d806f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path as P\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['KaiTi', 'SimHei', 'FangSong']  # 汉字字体,优先使用楷体，如果找不到楷体，则使用黑体\n",
    "plt.rcParams['font.size'] = 12  # 字体大小\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4207b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "train_size=0.9\n",
    "bs=16\n",
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2d95214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100cm湿度(kg/m2)</th>\n",
       "      <th>10cm湿度(kg/m2)</th>\n",
       "      <th>200cm湿度(kg/m2)</th>\n",
       "      <th>40cm湿度(kg/m2)</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>土壤蒸发量(mm)</th>\n",
       "      <th>平均气温(℃)</th>\n",
       "      <th>降水量(mm)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>0.008417</td>\n",
       "      <td>0.328250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.002568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-01</th>\n",
       "      <td>0.008417</td>\n",
       "      <td>0.269663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>0.113180</td>\n",
       "      <td>0.002568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01</th>\n",
       "      <td>0.008417</td>\n",
       "      <td>0.237560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042062</td>\n",
       "      <td>0.335609</td>\n",
       "      <td>0.024490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-01</th>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.186998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177738</td>\n",
       "      <td>0.626319</td>\n",
       "      <td>0.006840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-01</th>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.364366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.457297</td>\n",
       "      <td>0.785020</td>\n",
       "      <td>0.056964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>0.999609</td>\n",
       "      <td>0.342697</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.702094</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.030008</td>\n",
       "      <td>0.360852</td>\n",
       "      <td>0.595275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.280899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701157</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.225947</td>\n",
       "      <td>0.217316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.225522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701157</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.129112</td>\n",
       "      <td>0.681001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701157</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111732</td>\n",
       "      <td>0.892632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700844</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.146448</td>\n",
       "      <td>0.433892</td>\n",
       "      <td>0.129593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            100cm湿度(kg/m2)  10cm湿度(kg/m2)  200cm湿度(kg/m2)  40cm湿度(kg/m2)  \\\n",
       "time                                                                       \n",
       "2012-01-01        0.008417       0.328250        1.000000       0.036261   \n",
       "2012-02-01        0.008417       0.269663        1.000000       0.036261   \n",
       "2012-03-01        0.008417       0.237560        1.000000       0.036261   \n",
       "2012-04-01        0.007829       0.186998        1.000000       0.031885   \n",
       "2012-05-01        0.004306       0.364366        1.000000       0.008753   \n",
       "...                    ...            ...             ...            ...   \n",
       "2021-11-01        0.999609       0.342697        0.002451       0.702094   \n",
       "2021-12-01        1.000000       0.280899        0.000000       0.701157   \n",
       "2022-01-01        1.000000       0.225522        0.000000       0.701157   \n",
       "2022-02-01        1.000000       0.197432        0.000000       0.701157   \n",
       "2022-03-01        0.999804       0.426966        0.000000       0.700844   \n",
       "\n",
       "            month  year  土壤蒸发量(mm)   平均气温(℃)   降水量(mm)  \n",
       "time                                                    \n",
       "2012-01-01      1     0   0.008720  0.008070  0.002568  \n",
       "2012-02-01      2     0   0.015132  0.113180  0.002568  \n",
       "2012-03-01      3     0   0.042062  0.335609  0.024490  \n",
       "2012-04-01      4     0   0.177738  0.626319  0.006840  \n",
       "2012-05-01      5     0   0.457297  0.785020  0.056964  \n",
       "...           ...   ...        ...       ...       ...  \n",
       "2021-11-01     11     9   0.030008  0.360852  0.595275  \n",
       "2021-12-01     12     9   0.016927  0.225947  0.217316  \n",
       "2022-01-01      1    10   0.004360  0.129112  0.681001  \n",
       "2022-02-01      2    10   0.000000  0.111732  0.892632  \n",
       "2022-03-01      3    10   0.146448  0.433892  0.129593  \n",
       "\n",
       "[123 rows x 9 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('问题2-训练数据-归一化-回归.csv',index_col='time',parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "550eb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YCOLS=['100cm湿度(kg/m2)', '10cm湿度(kg/m2)', '200cm湿度(kg/m2)', '40cm湿度(kg/m2)',]\n",
    "XCOLS=['month', 'year', '土壤蒸发量(mm)', '平均气温(℃)', '降水量(mm)']\n",
    "\n",
    "X,Y=df[XCOLS].values,df[YCOLS].values\n",
    "\n",
    "X_train,X_val,Y_train,Y_val = train_test_split(X,Y, train_size=train_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "223f78fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110, 5), (13, 5), (110, 4), (13, 4))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_val.shape,Y_train.shape,Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1bed2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                384       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,804\n",
      "Trainable params: 4,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(X_train.shape[1], ))\n",
    "x = keras.layers.Dense(64, activation='tanh')(inputs)\n",
    "x = keras.layers.Dense(64, activation='tanh')(x)\n",
    "outputs = keras.layers.Dense(Y_train.shape[1])(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d6def6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/7 [===>..........................] - ETA: 3s - loss: 0.4251\n",
      "Epoch 1: val_loss improved from inf to 0.82933, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 1s 39ms/step - loss: 0.4022 - val_loss: 0.8293\n",
      "Epoch 2/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3605\n",
      "Epoch 2: val_loss improved from 0.82933 to 0.73458, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3015 - val_loss: 0.7346\n",
      "Epoch 3/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2013\n",
      "Epoch 3: val_loss improved from 0.73458 to 0.65771, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2259 - val_loss: 0.6577\n",
      "Epoch 4/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2434\n",
      "Epoch 4: val_loss improved from 0.65771 to 0.59052, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1758 - val_loss: 0.5905\n",
      "Epoch 5/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.1512\n",
      "Epoch 5: val_loss improved from 0.59052 to 0.53955, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.1397 - val_loss: 0.5396\n",
      "Epoch 6/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.1124\n",
      "Epoch 6: val_loss improved from 0.53955 to 0.49697, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1173 - val_loss: 0.4970\n",
      "Epoch 7/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0995\n",
      "Epoch 7: val_loss improved from 0.49697 to 0.45724, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1019 - val_loss: 0.4572\n",
      "Epoch 8/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0661\n",
      "Epoch 8: val_loss improved from 0.45724 to 0.42661, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0896 - val_loss: 0.4266\n",
      "Epoch 9/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0829\n",
      "Epoch 9: val_loss improved from 0.42661 to 0.39861, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0803 - val_loss: 0.3986\n",
      "Epoch 10/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0693\n",
      "Epoch 10: val_loss improved from 0.39861 to 0.37653, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0732 - val_loss: 0.3765\n",
      "Epoch 11/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0645\n",
      "Epoch 11: val_loss improved from 0.37653 to 0.35679, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0666 - val_loss: 0.3568\n",
      "Epoch 12/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0645\n",
      "Epoch 12: val_loss improved from 0.35679 to 0.33529, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0615 - val_loss: 0.3353\n",
      "Epoch 13/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0703\n",
      "Epoch 13: val_loss improved from 0.33529 to 0.31674, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0568 - val_loss: 0.3167\n",
      "Epoch 14/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0667\n",
      "Epoch 14: val_loss improved from 0.31674 to 0.30226, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0534 - val_loss: 0.3023\n",
      "Epoch 15/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0404\n",
      "Epoch 15: val_loss improved from 0.30226 to 0.29598, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0504 - val_loss: 0.2960\n",
      "Epoch 16/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0457\n",
      "Epoch 16: val_loss improved from 0.29598 to 0.28257, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0477 - val_loss: 0.2826\n",
      "Epoch 17/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0321\n",
      "Epoch 17: val_loss improved from 0.28257 to 0.27137, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0455 - val_loss: 0.2714\n",
      "Epoch 18/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0411\n",
      "Epoch 18: val_loss improved from 0.27137 to 0.26510, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0436 - val_loss: 0.2651\n",
      "Epoch 19/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0498\n",
      "Epoch 19: val_loss improved from 0.26510 to 0.25334, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0422 - val_loss: 0.2533\n",
      "Epoch 20/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0416\n",
      "Epoch 20: val_loss improved from 0.25334 to 0.24501, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0407 - val_loss: 0.2450\n",
      "Epoch 21/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0465\n",
      "Epoch 21: val_loss did not improve from 0.24501\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0394 - val_loss: 0.2456\n",
      "Epoch 22/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0269\n",
      "Epoch 22: val_loss improved from 0.24501 to 0.24233, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0382 - val_loss: 0.2423\n",
      "Epoch 23/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0387\n",
      "Epoch 23: val_loss improved from 0.24233 to 0.23479, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0372 - val_loss: 0.2348\n",
      "Epoch 24/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0285\n",
      "Epoch 24: val_loss improved from 0.23479 to 0.23007, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0363 - val_loss: 0.2301\n",
      "Epoch 25/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0391\n",
      "Epoch 25: val_loss improved from 0.23007 to 0.22935, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0354 - val_loss: 0.2293\n",
      "Epoch 26/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0308\n",
      "Epoch 26: val_loss improved from 0.22935 to 0.22641, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0346 - val_loss: 0.2264\n",
      "Epoch 27/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0301\n",
      "Epoch 27: val_loss improved from 0.22641 to 0.22339, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0339 - val_loss: 0.2234\n",
      "Epoch 28/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0216\n",
      "Epoch 28: val_loss improved from 0.22339 to 0.22121, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0333 - val_loss: 0.2212\n",
      "Epoch 29/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0330\n",
      "Epoch 29: val_loss improved from 0.22121 to 0.21731, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0327 - val_loss: 0.2173\n",
      "Epoch 30/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0339\n",
      "Epoch 30: val_loss improved from 0.21731 to 0.21436, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0322 - val_loss: 0.2144\n",
      "Epoch 31/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0318\n",
      "Epoch 31: val_loss did not improve from 0.21436\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0316 - val_loss: 0.2156\n",
      "Epoch 32/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0362\n",
      "Epoch 32: val_loss improved from 0.21436 to 0.21372, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0311 - val_loss: 0.2137\n",
      "Epoch 33/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0308\n",
      "Epoch 33: val_loss improved from 0.21372 to 0.21273, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0307 - val_loss: 0.2127\n",
      "Epoch 34/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0245\n",
      "Epoch 34: val_loss improved from 0.21273 to 0.21143, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0303 - val_loss: 0.2114\n",
      "Epoch 35/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0365\n",
      "Epoch 35: val_loss improved from 0.21143 to 0.20914, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0299 - val_loss: 0.2091\n",
      "Epoch 36/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0278\n",
      "Epoch 36: val_loss improved from 0.20914 to 0.20844, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0295 - val_loss: 0.2084\n",
      "Epoch 37/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0293\n",
      "Epoch 37: val_loss improved from 0.20844 to 0.20715, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0292 - val_loss: 0.2071\n",
      "Epoch 38/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0304\n",
      "Epoch 38: val_loss improved from 0.20715 to 0.20621, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0289 - val_loss: 0.2062\n",
      "Epoch 39/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0274\n",
      "Epoch 39: val_loss improved from 0.20621 to 0.20595, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0286 - val_loss: 0.2060\n",
      "Epoch 40/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0259\n",
      "Epoch 40: val_loss did not improve from 0.20595\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0284 - val_loss: 0.2068\n",
      "Epoch 41/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0274\n",
      "Epoch 41: val_loss improved from 0.20595 to 0.20419, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0281 - val_loss: 0.2042\n",
      "Epoch 42/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0196\n",
      "Epoch 42: val_loss improved from 0.20419 to 0.20016, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0280 - val_loss: 0.2002\n",
      "Epoch 43/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0289\n",
      "Epoch 43: val_loss did not improve from 0.20016\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0276 - val_loss: 0.2037\n",
      "Epoch 44/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0372\n",
      "Epoch 44: val_loss did not improve from 0.20016\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0275 - val_loss: 0.2029\n",
      "Epoch 45/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0213\n",
      "Epoch 45: val_loss did not improve from 0.20016\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0273 - val_loss: 0.2071\n",
      "Epoch 46/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0259\n",
      "Epoch 46: val_loss did not improve from 0.20016\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0271 - val_loss: 0.2054\n",
      "Epoch 47/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0357\n",
      "Epoch 47: val_loss improved from 0.20016 to 0.19706, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0269 - val_loss: 0.1971\n",
      "Epoch 48/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0243\n",
      "Epoch 48: val_loss did not improve from 0.19706\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0267 - val_loss: 0.1986\n",
      "Epoch 49/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0230\n",
      "Epoch 49: val_loss did not improve from 0.19706\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0265 - val_loss: 0.1972\n",
      "Epoch 50/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0227\n",
      "Epoch 50: val_loss improved from 0.19706 to 0.19418, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0264 - val_loss: 0.1942\n",
      "Epoch 51/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0369\n",
      "Epoch 51: val_loss did not improve from 0.19418\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.1957\n",
      "Epoch 52/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0391\n",
      "Epoch 52: val_loss did not improve from 0.19418\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0261 - val_loss: 0.1971\n",
      "Epoch 53/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0357\n",
      "Epoch 53: val_loss did not improve from 0.19418\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0260 - val_loss: 0.1970\n",
      "Epoch 54/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0264\n",
      "Epoch 54: val_loss did not improve from 0.19418\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0259 - val_loss: 0.1971\n",
      "Epoch 55/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0305\n",
      "Epoch 55: val_loss did not improve from 0.19418\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0258 - val_loss: 0.1959\n",
      "Epoch 56/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0265\n",
      "Epoch 56: val_loss improved from 0.19418 to 0.19240, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0256 - val_loss: 0.1924\n",
      "Epoch 57/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0261\n",
      "Epoch 57: val_loss did not improve from 0.19240\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0255 - val_loss: 0.1954\n",
      "Epoch 58/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0217\n",
      "Epoch 58: val_loss did not improve from 0.19240\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0254 - val_loss: 0.1942\n",
      "Epoch 59/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0180\n",
      "Epoch 59: val_loss improved from 0.19240 to 0.18964, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0253 - val_loss: 0.1896\n",
      "Epoch 60/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0191\n",
      "Epoch 60: val_loss improved from 0.18964 to 0.18930, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0253 - val_loss: 0.1893\n",
      "Epoch 61/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0232\n",
      "Epoch 61: val_loss did not improve from 0.18930\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0251 - val_loss: 0.1911\n",
      "Epoch 62/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0169\n",
      "Epoch 62: val_loss did not improve from 0.18930\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0251 - val_loss: 0.1923\n",
      "Epoch 63/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0328\n",
      "Epoch 63: val_loss did not improve from 0.18930\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0249 - val_loss: 0.1901\n",
      "Epoch 64/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0240\n",
      "Epoch 64: val_loss improved from 0.18930 to 0.18783, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0248 - val_loss: 0.1878\n",
      "Epoch 65/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0215\n",
      "Epoch 65: val_loss did not improve from 0.18783\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0248 - val_loss: 0.1899\n",
      "Epoch 66/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0273\n",
      "Epoch 66: val_loss did not improve from 0.18783\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0247 - val_loss: 0.1905\n",
      "Epoch 67/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0143\n",
      "Epoch 67: val_loss improved from 0.18783 to 0.18729, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0246 - val_loss: 0.1873\n",
      "Epoch 68/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0272\n",
      "Epoch 68: val_loss did not improve from 0.18729\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0245 - val_loss: 0.1876\n",
      "Epoch 69/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0227\n",
      "Epoch 69: val_loss improved from 0.18729 to 0.18468, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0245 - val_loss: 0.1847\n",
      "Epoch 70/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0304\n",
      "Epoch 70: val_loss did not improve from 0.18468\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0243 - val_loss: 0.1867\n",
      "Epoch 71/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0291\n",
      "Epoch 71: val_loss did not improve from 0.18468\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0243 - val_loss: 0.1870\n",
      "Epoch 72/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0260\n",
      "Epoch 72: val_loss did not improve from 0.18468\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0243 - val_loss: 0.1901\n",
      "Epoch 73/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0254\n",
      "Epoch 73: val_loss improved from 0.18468 to 0.18399, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0242 - val_loss: 0.1840\n",
      "Epoch 74/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0212\n",
      "Epoch 74: val_loss did not improve from 0.18399\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0241 - val_loss: 0.1843\n",
      "Epoch 75/500\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0254\n",
      "Epoch 75: val_loss did not improve from 0.18399\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0240 - val_loss: 0.1848\n",
      "Epoch 76/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0280\n",
      "Epoch 76: val_loss improved from 0.18399 to 0.18207, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0240 - val_loss: 0.1821\n",
      "Epoch 77/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0184\n",
      "Epoch 77: val_loss did not improve from 0.18207\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0239 - val_loss: 0.1846\n",
      "Epoch 78/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0177\n",
      "Epoch 78: val_loss did not improve from 0.18207\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.1821\n",
      "Epoch 79/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0282\n",
      "Epoch 79: val_loss did not improve from 0.18207\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0237 - val_loss: 0.1823\n",
      "Epoch 80/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0201\n",
      "Epoch 80: val_loss did not improve from 0.18207\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0237 - val_loss: 0.1837\n",
      "Epoch 81/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0329\n",
      "Epoch 81: val_loss improved from 0.18207 to 0.18122, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0236 - val_loss: 0.1812\n",
      "Epoch 82/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0281\n",
      "Epoch 82: val_loss improved from 0.18122 to 0.18053, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0235 - val_loss: 0.1805\n",
      "Epoch 83/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0137\n",
      "Epoch 83: val_loss did not improve from 0.18053\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0235 - val_loss: 0.1848\n",
      "Epoch 84/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0211\n",
      "Epoch 84: val_loss improved from 0.18053 to 0.17801, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0235 - val_loss: 0.1780\n",
      "Epoch 85/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0170\n",
      "Epoch 85: val_loss did not improve from 0.17801\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.1825\n",
      "Epoch 86/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0202\n",
      "Epoch 86: val_loss did not improve from 0.17801\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0233 - val_loss: 0.1822\n",
      "Epoch 87/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0203\n",
      "Epoch 87: val_loss did not improve from 0.17801\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0233 - val_loss: 0.1785\n",
      "Epoch 88/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0240\n",
      "Epoch 88: val_loss did not improve from 0.17801\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0232 - val_loss: 0.1783\n",
      "Epoch 89/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0146\n",
      "Epoch 89: val_loss did not improve from 0.17801\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0231 - val_loss: 0.1782\n",
      "Epoch 90/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0206\n",
      "Epoch 90: val_loss improved from 0.17801 to 0.17520, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0231 - val_loss: 0.1752\n",
      "Epoch 91/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0228\n",
      "Epoch 91: val_loss did not improve from 0.17520\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0230 - val_loss: 0.1797\n",
      "Epoch 92/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0185\n",
      "Epoch 92: val_loss did not improve from 0.17520\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0230 - val_loss: 0.1783\n",
      "Epoch 93/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0261\n",
      "Epoch 93: val_loss improved from 0.17520 to 0.17510, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0229 - val_loss: 0.1751\n",
      "Epoch 94/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0267\n",
      "Epoch 94: val_loss did not improve from 0.17510\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0229 - val_loss: 0.1785\n",
      "Epoch 95/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0311\n",
      "Epoch 95: val_loss improved from 0.17510 to 0.17234, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0229 - val_loss: 0.1723\n",
      "Epoch 96/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0198\n",
      "Epoch 96: val_loss did not improve from 0.17234\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.1769\n",
      "Epoch 97/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0291\n",
      "Epoch 97: val_loss improved from 0.17234 to 0.17231, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0228 - val_loss: 0.1723\n",
      "Epoch 98/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0200\n",
      "Epoch 98: val_loss did not improve from 0.17231\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0226 - val_loss: 0.1743\n",
      "Epoch 99/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0244\n",
      "Epoch 99: val_loss did not improve from 0.17231\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.1767\n",
      "Epoch 100/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0180\n",
      "Epoch 100: val_loss did not improve from 0.17231\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0225 - val_loss: 0.1761\n",
      "Epoch 101/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0208\n",
      "Epoch 101: val_loss did not improve from 0.17231\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0225 - val_loss: 0.1730\n",
      "Epoch 102/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0158\n",
      "Epoch 102: val_loss did not improve from 0.17231\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0224 - val_loss: 0.1756\n",
      "Epoch 103/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0404\n",
      "Epoch 103: val_loss improved from 0.17231 to 0.17163, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0225 - val_loss: 0.1716\n",
      "Epoch 104/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0208\n",
      "Epoch 104: val_loss did not improve from 0.17163\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.1718\n",
      "Epoch 105/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0236\n",
      "Epoch 105: val_loss improved from 0.17163 to 0.16608, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0224 - val_loss: 0.1661\n",
      "Epoch 106/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0186\n",
      "Epoch 106: val_loss did not improve from 0.16608\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0222 - val_loss: 0.1716\n",
      "Epoch 107/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0195\n",
      "Epoch 107: val_loss did not improve from 0.16608\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0222 - val_loss: 0.1746\n",
      "Epoch 108/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0157\n",
      "Epoch 108: val_loss did not improve from 0.16608\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0221 - val_loss: 0.1739\n",
      "Epoch 109/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0248\n",
      "Epoch 109: val_loss improved from 0.16608 to 0.16601, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0221 - val_loss: 0.1660\n",
      "Epoch 110/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0167\n",
      "Epoch 110: val_loss did not improve from 0.16601\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0220 - val_loss: 0.1672\n",
      "Epoch 111/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0226\n",
      "Epoch 111: val_loss did not improve from 0.16601\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0220 - val_loss: 0.1693\n",
      "Epoch 112/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0291\n",
      "Epoch 112: val_loss did not improve from 0.16601\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0219 - val_loss: 0.1711\n",
      "Epoch 113/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0354\n",
      "Epoch 113: val_loss improved from 0.16601 to 0.16441, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0219 - val_loss: 0.1644\n",
      "Epoch 114/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0210\n",
      "Epoch 114: val_loss did not improve from 0.16441\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 0.1667\n",
      "Epoch 115/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0222\n",
      "Epoch 115: val_loss did not improve from 0.16441\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0218 - val_loss: 0.1697\n",
      "Epoch 116/500\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 0.0206\n",
      "Epoch 116: val_loss did not improve from 0.16441\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0220 - val_loss: 0.1750\n",
      "Epoch 117/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0193\n",
      "Epoch 117: val_loss did not improve from 0.16441\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0217 - val_loss: 0.1646\n",
      "Epoch 118/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0195\n",
      "Epoch 118: val_loss did not improve from 0.16441\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0217 - val_loss: 0.1650\n",
      "Epoch 119/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0284\n",
      "Epoch 119: val_loss did not improve from 0.16441\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0217 - val_loss: 0.1670\n",
      "Epoch 120/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0231\n",
      "Epoch 120: val_loss improved from 0.16441 to 0.16282, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0216 - val_loss: 0.1628\n",
      "Epoch 121/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0173\n",
      "Epoch 121: val_loss did not improve from 0.16282\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0215 - val_loss: 0.1671\n",
      "Epoch 122/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0197\n",
      "Epoch 122: val_loss did not improve from 0.16282\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0214 - val_loss: 0.1696\n",
      "Epoch 123/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0160\n",
      "Epoch 123: val_loss did not improve from 0.16282\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.1643\n",
      "Epoch 124/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0241\n",
      "Epoch 124: val_loss did not improve from 0.16282\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0215 - val_loss: 0.1674\n",
      "Epoch 125/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0127\n",
      "Epoch 125: val_loss did not improve from 0.16282\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0213 - val_loss: 0.1641\n",
      "Epoch 126/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0203\n",
      "Epoch 126: val_loss did not improve from 0.16282\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0213 - val_loss: 0.1645\n",
      "Epoch 127/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0185\n",
      "Epoch 127: val_loss improved from 0.16282 to 0.16123, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0213 - val_loss: 0.1612\n",
      "Epoch 128/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0226\n",
      "Epoch 128: val_loss did not improve from 0.16123\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.1650\n",
      "Epoch 129/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0192\n",
      "Epoch 129: val_loss did not improve from 0.16123\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0212 - val_loss: 0.1657\n",
      "Epoch 130/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0286\n",
      "Epoch 130: val_loss did not improve from 0.16123\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0211 - val_loss: 0.1623\n",
      "Epoch 131/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0209\n",
      "Epoch 131: val_loss did not improve from 0.16123\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0211 - val_loss: 0.1615\n",
      "Epoch 132/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0151\n",
      "Epoch 132: val_loss did not improve from 0.16123\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0211 - val_loss: 0.1631\n",
      "Epoch 133/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0206\n",
      "Epoch 133: val_loss improved from 0.16123 to 0.16054, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0211 - val_loss: 0.1605\n",
      "Epoch 134/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0219\n",
      "Epoch 134: val_loss improved from 0.16054 to 0.15852, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0211 - val_loss: 0.1585\n",
      "Epoch 135/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0228\n",
      "Epoch 135: val_loss did not improve from 0.15852\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.1653\n",
      "Epoch 136/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0238\n",
      "Epoch 136: val_loss did not improve from 0.15852\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0210 - val_loss: 0.1656\n",
      "Epoch 137/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0194\n",
      "Epoch 137: val_loss did not improve from 0.15852\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0210 - val_loss: 0.1650\n",
      "Epoch 138/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0218\n",
      "Epoch 138: val_loss did not improve from 0.15852\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0209 - val_loss: 0.1639\n",
      "Epoch 139/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0218\n",
      "Epoch 139: val_loss improved from 0.15852 to 0.15779, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0208 - val_loss: 0.1578\n",
      "Epoch 140/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0228\n",
      "Epoch 140: val_loss did not improve from 0.15779\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0208 - val_loss: 0.1614\n",
      "Epoch 141/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0277\n",
      "Epoch 141: val_loss did not improve from 0.15779\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.1625\n",
      "Epoch 142/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0138\n",
      "Epoch 142: val_loss did not improve from 0.15779\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0206 - val_loss: 0.1600\n",
      "Epoch 143/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0357\n",
      "Epoch 143: val_loss did not improve from 0.15779\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0207 - val_loss: 0.1580\n",
      "Epoch 144/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0150\n",
      "Epoch 144: val_loss improved from 0.15779 to 0.15293, saving model to problem2_regress_checkpoint.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0207 - val_loss: 0.1529\n",
      "Epoch 145/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0177\n",
      "Epoch 145: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.1610\n",
      "Epoch 146/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0189\n",
      "Epoch 146: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0206 - val_loss: 0.1607\n",
      "Epoch 147/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0203\n",
      "Epoch 147: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0205 - val_loss: 0.1601\n",
      "Epoch 148/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0307\n",
      "Epoch 148: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.1607\n",
      "Epoch 149/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0254\n",
      "Epoch 149: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0205 - val_loss: 0.1539\n",
      "Epoch 150/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0225\n",
      "Epoch 150: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.1550\n",
      "Epoch 151/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0204\n",
      "Epoch 151: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0203 - val_loss: 0.1616\n",
      "Epoch 152/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0214\n",
      "Epoch 152: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0203 - val_loss: 0.1574\n",
      "Epoch 153/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0254\n",
      "Epoch 153: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.1561\n",
      "Epoch 154/500\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.0183\n",
      "Epoch 154: val_loss did not improve from 0.15293\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0203 - val_loss: 0.1598\n"
     ]
    }
   ],
   "source": [
    "path_checkpoint = \"problem2_regress_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=10)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    # 要同时保存网络结构。\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    "    batch_size=bs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e784dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e6e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7333d47a3a2e5eb003ed4862432b269301e4f62a805920b00eb5758860da8060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
